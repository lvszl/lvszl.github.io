>  本文摘自 [参考链接](https://zhuanlan.zhihu.com/p/265182368F)

Attention 被广泛用于序列到序列（seq2seq）模型，这是一种深度学习模型，在很多任务上都取得了成功，如：机器翻译、文本摘要、图像描述生成。谷歌翻译在 2016 年年末开始使用这种模型。有 2 篇开创性的论文(**[Sutskever et al., 2014](https://link.zhihu.com/?target=https%3A//papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf)**, **[Cho et al., 2014](https://link.zhihu.com/?target=http%3A//emnlp2014.org/papers/pdf/EMNLP2014179.pdf)**)对这些模型进行了解释。

然而，我发现，想要充分理解模型并实现它，需要深入理解一系列概念，而这些概念是层层递进的。我认为，如果能够把这些概念进行可视化，会更加有助于理解。这就是这篇文章的目标。当然你需要先了解一些深度学习的知识，才能读懂这篇文章。我希望这篇文章，可以对你理解上面提到的 2 篇论文有帮助。

一个典型的序列到序列（seq2seq）模型，接收的输入是一个（单词、字母、图像特征）序列，输出是另外一个序列。一个训练好的模型如下图所示：

![动图](https://pic2.zhimg.com/v2-621a247b57d0f72fd044d13f69f9d7cd_b.webp)

在神经机器翻译中，一个输入序列是指一连串的单词。类似地，输出也是一连串单词。

![动图](https://pic4.zhimg.com/v2-34172bf8ae2a28acaf53e328801a423f_b.webp)

\# 进一步理解细节

模型是由编码器（Encoder）和解码器（Decoder）组成的。其中，编码器会处理输入序列中的每个元素，把这些信息转换为一个向量（称为上下文（context））。当我们处理完整个输入序列后，编码器把上下文（context）发送给解码器，解码器开始逐项生成输出序列中的元素。

![动图](https://pic4.zhimg.com/v2-8e870ccf2859b24dd68f3fc0bc8fed87_b.webp)

这种机制，同样适用于机器翻译。

![动图](https://pic2.zhimg.com/v2-29b3d579bd44c9510e1753ea3a9dc759_b.webp)

在机器翻译任务中，上下文（context）是一个向量（基本上是由数字组成的数组）。编码器和解码器一般都是循环神经网络（你可以看看 Luis Serrano写 的 [一篇关于循环神经网络]([https://www.youtube.com/watch?v=UNmqTiOnRfg](https://link.zhihu.com/?target=https%3A//www.youtube.com/watch%3Fv%3DUNmqTiOnRfg)) 的精彩介绍）。

![](https://raw.githubusercontent.com/lvszl/figure/master/20231012152214.png)



上下文是一个浮点数向量。在下面，我们会可视化这些向量，使用更明亮的色彩来表示更大的值。 你可以在设置模型的时候设置**context向量的长度。这个长度就是编码器 RNN 的隐藏层神经元的数量**。上图的上下文向量长度为 4，但在实际应用中，上下文向量的长度可能是 256，512 或者 1024。

根据设计，RNN(encoder 和 decoder) 在每个时间步接受 2 个输入，包括：

- 输入序列中的一个元素（在解码器的例子中，输入是指句子中的一个单词）
- 一个 hidden state（隐藏层状态）

这里提到的单词都需要表示为一个向量。为了把一个词转换为一个向量(因此神经网络翻译本质上是将一组向量组成的序列变为另一组向量组成的序列)，我们使用一类称为词嵌入（Word Embedding） 的方法。这类方法把单词转换到一个向量空间，这种表示形式能够捕捉大量的单词的语义信息（例如，**[king - man + woman = queen](https://link.zhihu.com/?target=http%3A//p.migdal.pl/2017/01/06/king-man-woman-queen-why.html)**）。

![动图](https://pic3.zhimg.com/v2-697408d12207ea3e73b9c900ffd51d6a_b.webp)

RNN 在每个时间步，采用上一个时间步的 hidden state（隐藏层状态） 和当前时间步的输入向量，来得到输出。在下文，我们会使用类似的动画，来说明这些向量在神经机器翻译模型里的运作机制。

在下面的动画中，编码器和解码器在每个时间步处理输入，并得到输出。由于编码器和解码器都是 RNN，RNN 会根据当前时间步的输入，和前一个时间步的 hidden state（隐藏层状态），更新当前时间步的 hidden state（隐藏层状态）。

让我们看下编码器的 hidden state（隐藏层状态）。注意，最后一个 hidden state（隐藏层状态）实际上是我们传给解码器的上下文（context）。

（视频）

同样地，解码器也持有 hidden state（隐藏层状态），而且也需要把 hidden state（隐藏层状态）从一个时间步传递到下一个时间步。我们现在关注的是 RNN 的主要处理过程，因此没有在上图中可视化解码器的 hidden state，因为这个过程和解码器是类似的。

现在让我们用另一种方式来可视化序列到序列（seq2seq）模型。下面的动画会让我们更加容易理解模型。这种方法称为展开视图。其中，我们不只是显示一个解码器，而是在时间上展开，每个时间步都显示一个解码器。通过这种方式，我们可以看到每个时间步的输入和输出。

